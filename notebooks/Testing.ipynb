{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('/content/drive'):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL3FquH7hhLI"
   },
   "source": [
    "Lets test on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model():\n",
    "  print(\"-\"*50)\n",
    "  print(\"testing on the test dataset\")\n",
    "  print(\"-\"*50)\n",
    "  MODEL_PATH = \"/content/drive/MyDrive/best_model_hf\"\n",
    "  print(f\"loading model from path : {MODEL_PATH}\")\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  model = CLIPModel.from_pretrained(MODEL_PATH).to(device)\n",
    "  processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "  model.eval()\n",
    "  print(f\"Model successfully loaded for inferencing on device : {device}\")\n",
    "  print(\"\\n Load the test dataset\")\n",
    "  test_dataset = load_dataset(\"eltorio/ROCO-radiology\", split=\"test\")\n",
    "  print(\"\\n test data loaded\")\n",
    "  batch_size = 64\n",
    "  all_image_features = []\n",
    "  all_text_features = []\n",
    "  for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    batch_end = min(i+batch_size, len(test_dataset))\n",
    "    batch = test_dataset[i:batch_end]\n",
    "\n",
    "    images = [item.convert(\"RGB\") for item in batch[\"image\"]]\n",
    "    captions = batch[\"caption\"]\n",
    "\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\",padding=True).to(device)\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\",padding=True,truncation=True).to(device)\n",
    "\n",
    "    image_features = model.get_image_features(**image_inputs)\n",
    "    text_features = model.get_text_features(**text_inputs)\n",
    "    all_image_features.append(F.normalize(image_features,dim=-1))\n",
    "    all_text_features.append(F.normalize(text_features,dim=-1))\n",
    "  final_image_features = torch.cat(all_image_features,dim=0)\n",
    "  final_text_features = torch.cat(all_text_features,dim=0)\n",
    "\n",
    "  print(\"all features encoded\")\n",
    "\n",
    "  print(\"calculating similarity scores\")\n",
    "  similarity_scores = torch.matmul(final_image_features,final_text_features.t())\n",
    "  print(\"similarity scores calculated\")\n",
    "\n",
    "  print(\"calculating retrieval metrics\")\n",
    "  n = similarity_scores.shape[0]\n",
    "\n",
    "  img2txt_ranks = []\n",
    "  for i in tqdm(range(n), desc=\"Image→Text\"):\n",
    "    scores = similarity_scores[i]\n",
    "    rank = (scores.argsort(descending=True) == i).nonzero(as_tuple=True)[0].item()\n",
    "    img2txt_ranks.append(rank)\n",
    "\n",
    "  txt2img_ranks = []\n",
    "  for i in tqdm(range(n), desc=\"Text→Image\"):\n",
    "    scores = similarity_scores[:, i]\n",
    "    rank = (scores.argsort(descending=True) == i).nonzero(as_tuple=True)[0].item()\n",
    "    txt2img_ranks.append(rank)\n",
    "  img2txt_ranks = np.array(img2txt_ranks)\n",
    "  txt2img_ranks = np.array(txt2img_ranks)\n",
    "  metrics = {\n",
    "      'model_path': MODEL_PATH,\n",
    "      'test_samples': n,\n",
    "      'evaluation_date': datetime.now().isoformat(),\n",
    "      'image_to_text': {\n",
    "          'recall_at_1': float((img2txt_ranks < 1).mean() * 100),\n",
    "          'recall_at_5': float((img2txt_ranks < 5).mean() * 100),\n",
    "          'recall_at_10': float((img2txt_ranks < 10).mean() * 100),\n",
    "          'median_rank': float(np.median(img2txt_ranks) + 1),\n",
    "          'mean_rank': float(np.mean(img2txt_ranks) + 1),\n",
    "      },\n",
    "      'text_to_image': {\n",
    "          'recall_at_1': float((txt2img_ranks < 1).mean() * 100),\n",
    "          'recall_at_5': float((txt2img_ranks < 5).mean() * 100),\n",
    "          'recall_at_10': float((txt2img_ranks < 10).mean() * 100),\n",
    "          'median_rank': float(np.median(txt2img_ranks) + 1),\n",
    "          'mean_rank': float(np.mean(txt2img_ranks) + 1),\n",
    "      }\n",
    "    }\n",
    "  metrics['average_recall_at_5'] = (\n",
    "      metrics['image_to_text']['recall_at_5'] +\n",
    "      metrics['text_to_image']['recall_at_5']\n",
    "  ) / 2\n",
    "\n",
    "  print(\"\\n\" + \"=\"*70)\n",
    "  print(\" FINAL TEST SET RESULTS\")\n",
    "  print(\"=\"*70)\n",
    "\n",
    "  print(\"\\nImage → Text Retrieval:\")\n",
    "  print(f\"  R@1:  {metrics['image_to_text']['recall_at_1']:.2f}%\")\n",
    "  print(f\"  R@5:  {metrics['image_to_text']['recall_at_5']:.2f}%\")\n",
    "  print(f\"  R@10: {metrics['image_to_text']['recall_at_10']:.2f}%\")\n",
    "  print(f\"  Median Rank: {metrics['image_to_text']['median_rank']:.1f}\")\n",
    "\n",
    "  print(\"\\nText → Image Retrieval:\")\n",
    "  print(f\"  R@1:  {metrics['text_to_image']['recall_at_1']:.2f}%\")\n",
    "  print(f\"  R@5:  {metrics['text_to_image']['recall_at_5']:.2f}%\")\n",
    "  print(f\"  R@10: {metrics['text_to_image']['recall_at_10']:.2f}%\")\n",
    "  print(f\"  Median Rank: {metrics['text_to_image']['median_rank']:.1f}\")\n",
    "\n",
    "  print(f\"\\n Average R@5: {metrics['average_recall_at_5']:.2f}%\")\n",
    "\n",
    "  avg_r5 = metrics['average_recall_at_5']\n",
    "  if avg_r5 > 70:\n",
    "    print(\" super performance\")\n",
    "  elif avg_r5 > 60:\n",
    "    print(\" good performance!.\")\n",
    "  elif avg_r5 > 50:\n",
    "    print(\"  fair performance. Review needed\")\n",
    "  else:\n",
    "    print(\"  poor performance. retrain.\")\n",
    "\n",
    "  drive_dir = \"/content/drive/MyDrive/best_model_hf\"\n",
    "  results_file = os.path.join(drive_dir, \"evaluation_results.json\")\n",
    "\n",
    "  with open(results_file, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "  print(f\"\\n Metrics saved safely to: {results_file}\")\n",
    "  return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "testing on the test dataset\n",
      "--------------------------------------------------\n",
      "loading model from path : /content/drive/MyDrive/best_model_hf\n",
      "Model successfully loaded for inferencing on device : cuda\n",
      "\n",
      " Load the test dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3489aac7392c406a8c450f870a44b04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064cb9b9aed247ccbed1c1db95e4a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test data loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7296365b69824ab5b1a0f35c93e597e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all features encoded\n",
      "calculating similarity scores\n",
      "similarity scores calculated\n",
      "calculating retrieval metrics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb73fb6627a3498595ddf60bba31ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image→Text:   0%|          | 0/8176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2d89ddf7a848d9933c6618fffe7fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text→Image:   0%|          | 0/8176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " FINAL TEST SET RESULTS\n",
      "======================================================================\n",
      "\n",
      "Image → Text Retrieval:\n",
      "  R@1:  6.02%\n",
      "  R@5:  16.34%\n",
      "  R@10: 23.39%\n",
      "  Median Rank: 54.0\n",
      "\n",
      "Text → Image Retrieval:\n",
      "  R@1:  5.94%\n",
      "  R@5:  16.01%\n",
      "  R@10: 23.29%\n",
      "  Median Rank: 52.0\n",
      "\n",
      " Average R@5: 16.18%\n",
      "  poor performance. retrain.\n",
      "\n",
      " Metrics saved safely to: /content/drive/MyDrive/best_model_hf/evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /content/drive/MyDrive/best_model_hf...\n",
      "Loading Test Dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38f21db88204e979e08b6f7995647bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b12e570855c4ab9ad0842cbbb8d543f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Batch-wise Evaluation (Batch Size: 32)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356bb2a7fa5843959ce160749b9e3906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BATCH-WISE TEST RESULTS (N=8176)\n",
      "==================================================\n",
      "Recall@1: 70.83%\n",
      "Recall@5: 96.99%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "MODEL_PATH = \"/content/drive/MyDrive/best_model_hf\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = CLIPModel.from_pretrained(MODEL_PATH).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()\n",
    "\n",
    "print(\"Loading Test Dataset...\")\n",
    "test_dataset = load_dataset(\"eltorio/ROCO-radiology\", split=\"test\")\n",
    "\n",
    "batch_size = 32\n",
    "total_r1 = 0\n",
    "total_r5 = 0\n",
    "total_samples = 0\n",
    "\n",
    "print(f\"Running Batch-wise Evaluation (Batch Size: {batch_size})...\")\n",
    "\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    batch_end = min(i + batch_size, len(test_dataset))\n",
    "    batch = test_dataset[i:batch_end]\n",
    "\n",
    "    images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    captions = batch[\"caption\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=captions,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits_per_image\n",
    "    current_batch_size = logits.shape[0]\n",
    "    labels = torch.arange(current_batch_size, device=device)\n",
    "\n",
    "    pred = logits.argmax(dim=1)\n",
    "    total_r1 += (pred == labels).sum().item()\n",
    "\n",
    "    if current_batch_size >= 5:\n",
    "        _, top5_indices = logits.topk(5, dim=1)\n",
    "        total_r5 += (top5_indices == labels.view(-1, 1)).any(dim=1).sum().item()\n",
    "    else:\n",
    "        total_r5 += (pred == labels).sum().item()\n",
    "\n",
    "    total_samples += current_batch_size\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"BATCH-WISE TEST RESULTS (N={total_samples})\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Recall@1: {total_r1 / total_samples * 100:.2f}%\")\n",
    "print(f\"Recall@5: {total_r5 / total_samples * 100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
