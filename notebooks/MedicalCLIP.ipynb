{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import CLIPProcessor, CLIPModel, get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from typing import Literal, Optional, List\n",
    "from tqdm.auto import tqdm\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "  model_name = \"openai/clip-vit-base-patch32\"\n",
    "  dataset_name = \"eltorio/ROCO-radiology\"\n",
    "  max_length = 77\n",
    "  image_size = 224\n",
    "\n",
    "  strategy: Literal[\"vision_only\", \"text_only\", \"last_30\"] = \"last_30\"\n",
    "  num_epochs = 5\n",
    "  batch_size = 128\n",
    "  actual_batch_size = 32\n",
    "  gradient_accumulation_steps = 4\n",
    "  learning_rate = 1e-5\n",
    "  warmup_steps = 500\n",
    "  weight_decay = 0.01\n",
    "  max_grad_norm = 1.0\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  mixed_precision = True\n",
    "  num_workers = 2\n",
    "\n",
    "  use_wandb = True\n",
    "  checkpoint_dir = \"./checkpoints\"\n",
    "  save_every_n_epochs = 1\n",
    "  log_every_n_steps = 50\n",
    "\n",
    "  eval_every_n_epochs = 1\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCODataset(Dataset):\n",
    "    \"\"\"ROCO Radiology Dataset with CLIP preprocessing\"\"\"\n",
    "    def __init__(self, split=\"train\", processor=None, max_samples=None):\n",
    "        self.processor = processor\n",
    "\n",
    "        print(f\"Loading ROCO dataset ({split} split)...\")\n",
    "        dataset = load_dataset(config.dataset_name, split=split)\n",
    "\n",
    "        print(\"Filtering invalid samples...\")\n",
    "        original_len = len(dataset)\n",
    "\n",
    "        dataset = dataset.filter(\n",
    "            lambda x: x[\"image\"] is not None\n",
    "            and x[\"caption\"] is not None\n",
    "            and len(x[\"caption\"].strip()) > 0\n",
    "        )\n",
    "\n",
    "        print(f\"Filtered {original_len - len(dataset)} invalid samples.\")\n",
    "\n",
    "        if max_samples:\n",
    "            print(f\"Selecting first {max_samples} samples for debug...\")\n",
    "            dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "        self.data = dataset\n",
    "        print(f\"Final dataset size: {len(self.data)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        image = item['image']\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        caption = item['caption']\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=caption,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=config.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPFineTuner:\n",
    "\n",
    "  def __init__(self, model, strategy: str):\n",
    "    self.model = model\n",
    "    self.strategy = strategy\n",
    "    self.apply_strategy()\n",
    "\n",
    "  def freeze_all(self):\n",
    "    \"\"\"Freeze all parameters\"\"\"\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "  def get_layer_info(self):\n",
    "    \"\"\"Get information about model layers\"\"\"\n",
    "    vision_layers = []\n",
    "    text_layers = []\n",
    "\n",
    "    if hasattr(self.model.vision_model, 'encoder'):\n",
    "        vision_layers = list(self.model.vision_model.encoder.layers)\n",
    "    if hasattr(self.model.text_model, 'encoder'):\n",
    "        text_layers = list(self.model.text_model.encoder.layers)\n",
    "\n",
    "    return vision_layers, text_layers\n",
    "\n",
    "  def apply_strategy(self):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Applying Fine-tuning Strategy: {self.strategy.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    self.freeze_all()\n",
    "\n",
    "    if self.strategy == \"vision_only\":\n",
    "        self._apply_vision_only()\n",
    "    elif self.strategy == \"text_only\":\n",
    "        self._apply_text_only()\n",
    "    elif self.strategy == \"last_30\":\n",
    "        self._apply_last_30()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "\n",
    "    self._print_trainable_params()\n",
    "\n",
    "  def _apply_vision_only(self):\n",
    "    print(\"Unfreezing: Vision Encoder\")\n",
    "    for param in self.model.vision_model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    if hasattr(self.model, 'visual_projection'):\n",
    "        for param in self.model.visual_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "  def _apply_text_only(self):\n",
    "    print(\"Unfreezing: Text Encoder\")\n",
    "    for param in self.model.text_model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    if hasattr(self.model, 'text_projection'):\n",
    "        for param in self.model.text_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "  def _apply_last_30(self):\n",
    "    \"\"\"Unfreeze last 30% of layers in both encoders\"\"\"\n",
    "    vision_layers, text_layers = self.get_layer_info()\n",
    "\n",
    "    vision_threshold = int(len(vision_layers) * 0.7)\n",
    "    text_threshold = int(len(text_layers) * 0.7)\n",
    "\n",
    "    print(f\"Vision Encoder: {len(vision_layers)} layers total\")\n",
    "    print(f\"  - Freezing first {vision_threshold} layers\")\n",
    "    print(f\"  - Unfreezing last {len(vision_layers) - vision_threshold} layers\")\n",
    "\n",
    "    print(f\"\\nText Encoder: {len(text_layers)} layers total\")\n",
    "    print(f\"  - Freezing first {text_threshold} layers\")\n",
    "    print(f\"  - Unfreezing last {len(text_layers) - text_threshold} layers\")\n",
    "\n",
    "    for i in range(vision_threshold, len(vision_layers)):\n",
    "        for param in vision_layers[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(self.model.vision_model, 'post_layernorm'):\n",
    "        print(\"  - Unfreezing Vision Post-LayerNorm\")\n",
    "        for param in self.model.vision_model.post_layernorm.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    for i in range(text_threshold, len(text_layers)):\n",
    "        for param in text_layers[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(self.model.text_model, 'final_layer_norm'):\n",
    "          print(\"  - Unfreezing Text Final-LayerNorm\")\n",
    "          for param in self.model.text_model.final_layer_norm.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(self.model, 'visual_projection'):\n",
    "        for param in self.model.visual_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(self.model, 'text_projection'):\n",
    "        for param in self.model.text_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "  def _print_trainable_params(self):\n",
    "    \"\"\"Print summary of trainable parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in self.model.parameters())\n",
    "    percentage = 100 * trainable / total\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Trainable Parameters: {trainable:,} / {total:,} ({percentage:.2f}%)\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTrainer:\n",
    "  \"\"\"Trainer for CLIP fine-tuning\"\"\"\n",
    "\n",
    "  def __init__(self, model, train_loader, val_loader, config):\n",
    "    self.model = model.to(config.device)\n",
    "    self.train_loader = train_loader\n",
    "    self.val_loader = val_loader\n",
    "    self.config = config\n",
    "    trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "    self.optimizer = torch.optim.AdamW(\n",
    "        trainable_params,\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    num_update_steps_per_epoch = len(train_loader) // config.gradient_accumulation_steps\n",
    "    max_train_steps = config.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    self.scheduler = get_cosine_schedule_with_warmup(\n",
    "        self.optimizer,\n",
    "        num_warmup_steps=config.warmup_steps,\n",
    "        num_training_steps=max_train_steps\n",
    "    )\n",
    "    self.scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None\n",
    "\n",
    "    self.global_step = 0\n",
    "    self.best_val_loss = float('inf')\n",
    "\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "  def train_epoch(self,epoch):\n",
    "    self.model.train()\n",
    "    total_loss = 0\n",
    "    total_grad_norm = 0\n",
    "\n",
    "    pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(pbar):\n",
    "      pixel_values = batch['pixel_values'].to(self.config.device)\n",
    "      input_ids = batch['input_ids'].to(self.config.device)\n",
    "      attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "\n",
    "      if self.scaler:\n",
    "          with torch.cuda.amp.autocast():\n",
    "              outputs = self.model(\n",
    "                  pixel_values=pixel_values,\n",
    "                  input_ids=input_ids,\n",
    "                  attention_mask=attention_mask,\n",
    "                  return_loss=True\n",
    "              )\n",
    "\n",
    "              loss = outputs.loss / self.config.gradient_accumulation_steps\n",
    "\n",
    "          self.scaler.scale(loss).backward()\n",
    "\n",
    "      else:\n",
    "          outputs = self.model(\n",
    "              pixel_values=pixel_values,\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              return_loss=True\n",
    "          )\n",
    "          loss = outputs.loss / self.config.gradient_accumulation_steps\n",
    "          loss.backward()\n",
    "      current_loss = loss.item()*self.config.gradient_accumulation_steps\n",
    "      total_loss += current_loss\n",
    "\n",
    "      if (step + 1) % self.config.gradient_accumulation_steps == 0 or (step+1)==len(self.train_loader):\n",
    "          if self.scaler:\n",
    "              self.scaler.unscale_(self.optimizer)\n",
    "          grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                  self.model.parameters(),\n",
    "                  self.config.max_grad_norm\n",
    "              )\n",
    "          if self.scaler:\n",
    "              self.scaler.step(self.optimizer)\n",
    "              self.scaler.update()\n",
    "          else:\n",
    "              self.optimizer.step()\n",
    "\n",
    "          self.optimizer.zero_grad()\n",
    "          self.scheduler.step()\n",
    "          self.global_step += 1\n",
    "\n",
    "          if self.config.use_wandb and self.global_step % self.config.log_every_n_steps == 0:\n",
    "                wandb.log({\n",
    "                  'train/loss': current_loss,\n",
    "                  'train/grad_norm': grad_norm.item(),\n",
    "                  'train/learning_rate': self.scheduler.get_last_lr()[0],\n",
    "                  'global_step': self.global_step\n",
    "              })\n",
    "      pbar.set_postfix({'loss':f\"{current_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(self.train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def validate(self):\n",
    "    \"\"\"Validate with Recall@1 and Recall@5\"\"\"\n",
    "    self.model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct_r1 = 0\n",
    "    total_correct_r5 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        pixel_values = batch['pixel_values'].to(self.config.device)\n",
    "        input_ids = batch['input_ids'].to(self.config.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.config.device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):\n",
    "            outputs = self.model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_loss=True\n",
    "            )\n",
    "        logits = outputs.logits_per_image\n",
    "        batch_size = logits.shape[0]\n",
    "        labels = torch.arange(batch_size, device=self.config.device)\n",
    "\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total_correct_r1 += (pred == labels).sum().item()\n",
    "\n",
    "        if batch_size >= 5:\n",
    "            _, top5_indices = logits.topk(5, dim=1)\n",
    "            total_correct_r5 += (top5_indices == labels.view(-1, 1)).any(dim=1).sum().item()\n",
    "        else:\n",
    "            total_correct_r5 += (pred==labels).sum().item()\n",
    "\n",
    "        total_loss += outputs.loss.item()\n",
    "        total_samples += batch_size\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_r1 = total_correct_r1 / total_samples\n",
    "        avg_r5 = total_correct_r5 / total_samples\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{avg_loss:.4f}\",\n",
    "            'r1' : f\"{avg_r1:.4f}\",\n",
    "            'r5' : f\"{avg_r5:.4f}\"\n",
    "        })\n",
    "\n",
    "    return avg_loss, avg_r1, avg_r5\n",
    "\n",
    "  def save_checkpoint(self, epoch, val_loss, is_best=False):\n",
    "      \"\"\"Save model checkpoint\"\"\"\n",
    "\n",
    "      checkpoint = {\n",
    "          'epoch': epoch,\n",
    "          'model_state_dict': self.model.state_dict(),\n",
    "          'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "          'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "          'val_loss': val_loss,\n",
    "          'global_step': self.global_step,\n",
    "          'config': vars(self.config)\n",
    "      }\n",
    "      path = os.path.join(\n",
    "          self.config.checkpoint_dir,\n",
    "          f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "      )\n",
    "      torch.save(checkpoint, path)\n",
    "      print(f\"Saved checkpoint: {path}\")\n",
    "\n",
    "      if is_best:\n",
    "          best_pt_path = os.path.join(self.config.checkpoint_dir, \"best_model.pt\")\n",
    "          torch.save(checkpoint, best_pt_path)\n",
    "          hf_save_dir = os.path.join(self.config.checkpoint_dir, \"best_model_hf\")\n",
    "          self.model.save_pretrained(hf_save_dir)\n",
    "          print(f\"Saved best model (HF Ready): {hf_save_dir}\")\n",
    "\n",
    "  def train(self):\n",
    "      \"\"\"Main training loop\"\"\"\n",
    "      print(\"\\n\" + \"=\"*60)\n",
    "      print(\"Starting Training\")\n",
    "      print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "      for epoch in range(self.config.num_epochs):\n",
    "\n",
    "          train_loss= self.train_epoch(epoch)\n",
    "\n",
    "          print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs}\")\n",
    "          print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "          val_loss = None\n",
    "          is_best = False\n",
    "\n",
    "          if (epoch + 1) % self.config.eval_every_n_epochs == 0:\n",
    "\n",
    "              val_loss, val_r1, val_r5 = self.validate()\n",
    "\n",
    "              print(f\"Val Loss: {val_loss:.4f} | R@1: {val_r1:.4f} | R@5: {val_r5:.4f}\")\n",
    "\n",
    "              if self.config.use_wandb:\n",
    "                  wandb.log({\n",
    "                      'val/loss': val_loss,\n",
    "                      'val/recall_1': val_r1,\n",
    "                      'val/recall_5': val_r5,\n",
    "                      'epoch': epoch + 1\n",
    "                  })\n",
    "\n",
    "              if val_loss < self.best_val_loss:\n",
    "                  self.best_val_loss = val_loss\n",
    "                  is_best = True\n",
    "                  print(\"New best model found!\")\n",
    "\n",
    "          should_save = ((epoch + 1) % self.config.save_every_n_epochs == 0) or is_best\n",
    "\n",
    "          if should_save:\n",
    "              save_loss = val_loss if val_loss is not None else self.best_val_loss\n",
    "              self.save_checkpoint(epoch, save_loss, is_best)\n",
    "\n",
    "          print()\n",
    "\n",
    "  def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load a checkpoint to resume training\"\"\"\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        self.global_step = checkpoint['global_step']\n",
    "        self.best_val_loss = checkpoint['val_loss']\n",
    "\n",
    "        print(f\"Resuming from Epoch {start_epoch}\")\n",
    "        return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "import glob\n",
    "\n",
    "def main():\n",
    "    DEBUG = False\n",
    "    RESUME_FROM = None\n",
    "\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "    drive_checkpoint_dir = \"/content/drive/MyDrive/clip_roco_finetuning\"\n",
    "    config.checkpoint_dir = drive_checkpoint_dir\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"CLIP Fine-tuning on ROCO (Production Mode)\")\n",
    "    print(f\"Checkpoints will save to: {config.checkpoint_dir}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if config.use_wandb:\n",
    "        run_name = f\"clip_full_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        if DEBUG: run_name += \"_DEBUG\"\n",
    "\n",
    "        wandb.init(\n",
    "            project=\"clip-roco-finetuning\",\n",
    "            config=vars(config),\n",
    "            name=run_name,\n",
    "            resume=\"allow\"\n",
    "        )\n",
    "\n",
    "    print(\"Loading CLIP...\")\n",
    "    processor = CLIPProcessor.from_pretrained(config.model_name)\n",
    "    model = CLIPModel.from_pretrained(config.model_name)\n",
    "\n",
    "    CLIPFineTuner(model, config.strategy)\n",
    "\n",
    "    print(\"Loading Datasets...\")\n",
    "    train_limit = 100 if DEBUG else None\n",
    "    val_limit = 50 if DEBUG else None\n",
    "\n",
    "    train_dataset = ROCODataset(split=\"train\", processor=processor, max_samples=train_limit)\n",
    "    val_dataset = ROCODataset(split=\"validation\", processor=processor, max_samples=val_limit)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.actual_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.actual_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    trainer = CLIPTrainer(model, train_loader, val_loader, config)\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    if RESUME_FROM and os.path.exists(RESUME_FROM):\n",
    "        print(f\"Resuming from: {RESUME_FROM}\")\n",
    "        start_epoch = trainer.load_checkpoint(RESUME_FROM)\n",
    "\n",
    "    print(f\"Starting Training from Epoch {start_epoch+1}...\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, config.num_epochs):\n",
    "\n",
    "            train_loss= trainer.train_epoch(epoch)\n",
    "            print(f\"\\nEpoch {epoch+1}/{config.num_epochs} | Loss: {train_loss:.4f}\")\n",
    "\n",
    "            if (epoch + 1) % config.eval_every_n_epochs == 0:\n",
    "                val_loss, val_r1, val_r5 = trainer.validate()\n",
    "                print(f\"Val Loss: {val_loss:.4f} | R@1: {val_r1:.4f} | R@5: {val_r5:.4f}\")\n",
    "\n",
    "                if config.use_wandb:\n",
    "                    wandb.log({\n",
    "                        'val/loss': val_loss,\n",
    "                        'val/recall_1': val_r1,\n",
    "                        'val/recall_5': val_r5,\n",
    "                        'epoch': epoch+1\n",
    "                    })\n",
    "\n",
    "                is_best = val_loss < trainer.best_val_loss\n",
    "                if is_best:\n",
    "                    trainer.best_val_loss = val_loss\n",
    "                    print(\"New best model found!\")\n",
    "\n",
    "                if ((epoch + 1) % config.save_every_n_epochs == 0) or is_best:\n",
    "                    trainer.save_checkpoint(epoch, val_loss, is_best)\n",
    "\n",
    "            print()\n",
    "\n",
    "        print(\"\\n Full Training Completed Successfully!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Training interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Error during training: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if config.use_wandb:\n",
    "            wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLIP Fine-tuning on ROCO (Production Mode)\n",
      "Checkpoints will save to: /content/drive/MyDrive/clip_roco_finetuning\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251227_134742-mjfev9ck</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning/runs/mjfev9ck' target=\"_blank\">clip_full_20251227_134742</a></strong> to <a href='https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning' target=\"_blank\">https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning/runs/mjfev9ck' target=\"_blank\">https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning/runs/mjfev9ck</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83611623f1354efbba6bd4b2dfc37c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d935217ab92d4a008ccfd5390584aa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd9bd66f6234d3886dfe11a9f72b5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c4603b37bf4b8299f632f237c40e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74357bdb50346bdb94ed9f1fcd209eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d231ca057844c48ae46b3107614e35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c41f7b59b8d4c80a4309eee363b692d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dfdf118e524e659900b4a5e22f861f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Applying Fine-tuning Strategy: LAST_30\n",
      "============================================================\n",
      "\n",
      "Vision Encoder: 12 layers total\n",
      "  - Freezing first 8 layers\n",
      "  - Unfreezing last 4 layers\n",
      "\n",
      "Text Encoder: 12 layers total\n",
      "  - Freezing first 8 layers\n",
      "  - Unfreezing last 4 layers\n",
      "  - Unfreezing Vision Post-LayerNorm\n",
      "  - Unfreezing Text Final-LayerNorm\n",
      "\n",
      "============================================================\n",
      "Trainable Parameters: 41,618,944 / 151,277,313 (27.51%)\n",
      "============================================================\n",
      "\n",
      "Loading Datasets...\n",
      "Loading ROCO dataset (train split)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f728ee756b448cbeb2e035f7e12988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e891564773493ea1b729d9c14edffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef2320adc6344f291c1513afd296d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfabc6110e294db28444d9a8bb3da32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04babd0acdc94b33a43ba887c139fc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/26 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ae2dcf2fe74a06b4a4a5046b19fb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00026.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17c5cb461fb4375a362161d8fca9f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00026.parquet:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9542a8b850fe4e83808b797cb3f84cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00026.parquet:   0%|          | 0.00/489M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14a59d6088a4b669bad330e8289c2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00026.parquet:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c435b013b6f149e2a9a9d4d6b41944de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00026.parquet:   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dad31e017a439ea7d414df74a2a49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00026.parquet:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62944bd8d02e4209a000f191f2a821f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00026.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4b9f86d814493b9337afa70e197554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00026.parquet:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62021fea97948338e27a1c3b1e81c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00026.parquet:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd62a3063b4ffa95e3d7501a72b9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00026.parquet:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4b4c55cb4c4cb0be5de937d8ffd7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00026.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6995313251342f6a6475577d76612ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00011-of-00026.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5873be8bc92042108fe241fe712e4963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00012-of-00026.parquet:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730625e622ea4b719ecabdb618be7a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00013-of-00026.parquet:   0%|          | 0.00/493M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009abf2f68be416ebe887d4b442fa3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00014-of-00026.parquet:   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7626cb9364f349cdbb2ccc53729579e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00015-of-00026.parquet:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86635b71d1db4128b4552df6cbffb116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00016-of-00026.parquet:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa611b659864c37a0cf4663081a94c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00017-of-00026.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f975e72694f3444eabc9c8a8f12c9ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00018-of-00026.parquet:   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6474353af98845a58808b46ec4cd79d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00019-of-00026.parquet:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6f15f5912e44208a62c5121b7ececb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00020-of-00026.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2c21223cac4a639a8c883130fe7a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00021-of-00026.parquet:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e572e3d7eb11423a970e24e064cb870d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00022-of-00026.parquet:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca4ef5584ed48d891e5f5b5a3c273a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00023-of-00026.parquet:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb731d211384d168a778d6612a462ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00024-of-00026.parquet:   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b77f7f26a946dda9fae16b4834c6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00025-of-00026.parquet:   0%|          | 0.00/506M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2faa0a7a3c4becafc867eb96234c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/276M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66580053fc4a484595e5c720001b2459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b36bb93ee0044bb94622539887aa0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/65423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6e41a4d18445ab81f48012b47e6e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/8175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aac6b10934747fd8ac385424b57fb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa74542a5354421a553d293aea76bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering invalid samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ba9ea9674a40ccab7b1714a895f61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/65423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 invalid samples.\n",
      "Final dataset size: 65422 samples\n",
      "Loading ROCO dataset (validation split)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b92c1bec4fd40ff9ad035b333553909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1a5b7c8f8e4bffaa426ca037a5b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering invalid samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9478467b26cb411eb45031a01ad7019f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 0 invalid samples.\n",
      "Final dataset size: 8175 samples\n",
      "Starting Training from Epoch 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe324e4feb84862afabffaca36b2789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/2044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 | Loss: 1.5546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7895c967aef246169ee7351ef93dcee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1890 | R@1: 0.5862 | R@5: 0.9412\n",
      "New best model found!\n",
      "Saved checkpoint: /content/drive/MyDrive/clip_roco_finetuning/checkpoint_epoch_1.pt\n",
      "Saved best model (HF Ready): /content/drive/MyDrive/clip_roco_finetuning/best_model_hf\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7b4ca51b9b4532b2ee6befbe2e29b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/2044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5 | Loss: 0.9457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6537421165164e6a9289bf9ca450a601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9569 | R@1: 0.6678 | R@5: 0.9646\n",
      "New best model found!\n",
      "Saved checkpoint: /content/drive/MyDrive/clip_roco_finetuning/checkpoint_epoch_2.pt\n",
      "Saved best model (HF Ready): /content/drive/MyDrive/clip_roco_finetuning/best_model_hf\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4baf07586c0d475cb56b451c6155363b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/2044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5 | Loss: 0.6480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f166b55d8f744fb2b6328c6a9d3204ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9046 | R@1: 0.7015 | R@5: 0.9715\n",
      "New best model found!\n",
      "Saved checkpoint: /content/drive/MyDrive/clip_roco_finetuning/checkpoint_epoch_3.pt\n",
      "Saved best model (HF Ready): /content/drive/MyDrive/clip_roco_finetuning/best_model_hf\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b999bcdd8d284717bfb0956002b6c00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/2044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5 | Loss: 0.4581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc0babbcf1745139c3a0583eca659a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9177 | R@1: 0.7059 | R@5: 0.9695\n",
      "Saved checkpoint: /content/drive/MyDrive/clip_roco_finetuning/checkpoint_epoch_4.pt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0268fdfdbab9429a8d681aab49132c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/2044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5 | Loss: 0.3751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56221a192d074b928ad739e8a56904c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9385 | R@1: 0.7048 | R@5: 0.9709\n",
      "Saved checkpoint: /content/drive/MyDrive/clip_roco_finetuning/checkpoint_epoch_5.pt\n",
      "\n",
      "\n",
      " Full Training Completed Successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>val/loss</td><td></td></tr><tr><td>val/recall_1</td><td></td></tr><tr><td>val/recall_5</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>global_step</td><td>2550</td></tr><tr><td>train/grad_norm</td><td>21.14726</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.53771</td></tr><tr><td>val/loss</td><td>0.9385</td></tr><tr><td>val/recall_1</td><td>0.70483</td></tr><tr><td>val/recall_5</td><td>0.97089</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clip_full_20251227_134742</strong> at: <a href='https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning/runs/mjfev9ck' target=\"_blank\">https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning/runs/mjfev9ck</a><br> View project at: <a href='https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning' target=\"_blank\">https://wandb.ai/mitukhandal01-birla-institute-of-technology-mesra/clip-roco-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251227_134742-mjfev9ck/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
